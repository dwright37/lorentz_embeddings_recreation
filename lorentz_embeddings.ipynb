{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# WordNet dataset generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataForSynset(sname, fname):\n",
    "\n",
    "    closure = defaultdict(set)\n",
    "\n",
    "    def walk(node, ancestors):\n",
    "        node_name = node.name()\n",
    "        closure[node_name].update(ancestors)\n",
    "        for s in node.hyponyms():\n",
    "            walk(s, ancestors + [node_name])\n",
    "\n",
    "    walk(wn.synset(sname), [sname])\n",
    "    #Write out tsv\n",
    "    with open(fname, 'w') as f:\n",
    "        for n in closure:\n",
    "            for a in closure[n]:\n",
    "                f.write(n + '\\t' + a + '\\t' + \"1\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getDataForSynset('entity.n.01', \"data/wordnet_nouns.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getDataForSynset('mammal.n.01', \"data/wordnet_mammals.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import timeit\n",
    "import gc\n",
    "import logging\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.multiprocessing as mp\n",
    "from torch.autograd import Variable\n",
    "from collections import defaultdict as ddict\n",
    "from sklearn.metrics import average_precision_score\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from model import LorentzEmbedding\n",
    "from data import DatasetReader\n",
    "from optimization import RiemannianSGD\n",
    "from optimization import LorentzDistance\n",
    "\n",
    "th.set_default_tensor_type(th.DoubleTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(types, model):\n",
    "    with th.no_grad():\n",
    "        embs = th.from_numpy(model.embedding())\n",
    "        embedding = Variable(embs)\n",
    "        ranks = []\n",
    "        ap_scores = []\n",
    "        for s, tree in types.items():\n",
    "            s_e = Variable(embs[s].unsqueeze(0))\n",
    "            dists_curr = model.dist()(s_e, embedding).data.cpu().numpy().flatten()\n",
    "            dists_curr[s] = 1e14\n",
    "            labels = np.zeros(embedding.size(0))\n",
    "            dists_masked = dists_curr.copy()\n",
    "            ranks_curr = []\n",
    "            \n",
    "            for o in tree:\n",
    "                dists_masked[o] = float('inf')\n",
    "                labels[o] = 1 \n",
    "            ap_scores.append(average_precision_score(labels, -dists_curr))\n",
    "            for o in tree:\n",
    "                d = dists_masked.copy()\n",
    "                d[o] = dists_curr[o]\n",
    "                r = np.argsort(d)\n",
    "                ranks_curr.append(np.where(r == o)[0][0] + 1)\n",
    "            ranks += ranks_curr\n",
    "    return np.mean(ranks), np.mean(ap_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_lr_multiplier = 0.1                     #Burnin multiplier\n",
    "dim = 3                                  #The dimensionality of the embdding\n",
    "dataset = './data/wordnet_mammals.tsv'   #The dataset to learn\n",
    "fout = './mammals.pth'                   #The output model file\n",
    "lr_base = 1.0                            #The base learning rate\n",
    "epochs = 1500                            #Max number of epochs\n",
    "batchsize = 20                           #The batch size\n",
    "negs = 50                                #Number of negative examples\n",
    "eval_each = 10                           #The number of epochs between each evaluation\n",
    "burnin = 20                              #The number of burnin epochs\n",
    "nworkers = 5                             #The number of dataset readers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset setup\n",
    "data = DatasetReader(dataset, negs)\n",
    "idx = data.samples\n",
    "\n",
    "# create adjacency list for evaluation\n",
    "adjacency = ddict(set)\n",
    "for i in range(len(idx)):\n",
    "    s, o, _ = idx[i]\n",
    "    adjacency[s].add(o)\n",
    "adjacency = dict(adjacency)\n",
    "\n",
    "# initialize model and data\n",
    "model = LorentzEmbedding(len(data.entities), dim, LorentzDistance)\n",
    "\n",
    "# initialize optimizer\n",
    "optimizer = RiemannianSGD(\n",
    "    model.parameters(),\n",
    "    lr=lr_base,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_rank = (float('inf'), -1)\n",
    "max_map = (0, -1)\n",
    "loader = DataLoader(\n",
    "        data,\n",
    "        batch_size=batchsize,\n",
    "        shuffle=True,\n",
    "        num_workers=nworkers,\n",
    "        collate_fn=data.collate\n",
    "    )\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = []\n",
    "    loss = None\n",
    "    data.burnin = False\n",
    "    lr = lr_base\n",
    "    t_start = timeit.default_timer()\n",
    "    if epoch < burnin:\n",
    "        data.burnin = True\n",
    "        lr = lr_base * _lr_multiplier\n",
    "        \n",
    "    #Training loop\n",
    "    for inputs, targets in loader:\n",
    "        inputs = Variable(th.from_numpy(np.vstack(inputs)))\n",
    "        targets = Variable(th.from_numpy(np.vstack(targets))).squeeze()\n",
    "\n",
    "        elapsed = timeit.default_timer() - t_start\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(inputs)\n",
    "        loss = model.loss(preds, targets, size_average=True)\n",
    "        loss.backward()\n",
    "        optimizer.step(lr=lr)\n",
    "        epoch_loss.append(loss.data[0])\n",
    "        th.save({\n",
    "           'model': model.state_dict(),\n",
    "           'epoch': epoch,\n",
    "           'entities': data.entities\n",
    "        }, 'run/%05d.pth'%epoch)\n",
    "        \n",
    "    #Evaluation\n",
    "    if epoch == (epochs - 1) or epoch % eval_each == (eval_each - 1):\n",
    "        th.save({\n",
    "                'model': model.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'entities': data.entities\n",
    "            }, fout)\n",
    "        mrank, mAP = evaluate(adjacency, model)\n",
    "        if mrank < min_rank[0]:\n",
    "            min_rank = (mrank, epoch)\n",
    "        if mAP > max_map[0]:\n",
    "            max_map = (mAP, epoch)\n",
    "        print(\n",
    "            ('epoch: %d, '\n",
    "             'loss: %.3f, '\n",
    "             'mean rank: %.2f, '\n",
    "             'mAP: %.4f, '\n",
    "             'best rank: %.2f, '\n",
    "             'best mAP: %.4f, '\n",
    "             'time: %.2fs') % (\n",
    "                 epoch, loss, mrank, mAP, min_rank[0], max_map[0], elapsed)\n",
    "        )\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = th.load('./mammals.pth')\n",
    "lorentz_embeddings = model['model']['embeddings.weight']\n",
    "dim0 = lorentz_embeddings[:,0].unsqueeze(1)\n",
    "dimn = lorentz_embeddings[:,1:]\n",
    "\n",
    "poincare_embeddings = dimn / (dim0 + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = defaultdict(set)\n",
    "family_start = ['lynx.n.02']\n",
    "for f in family_start:\n",
    "    stack = [f]\n",
    "    first = True\n",
    "    while len(stack) > 0:\n",
    "        v = stack.pop()\n",
    "        for k in wn.synsets(v.split('.')[0]):\n",
    "            if k.name() in model['entities']:\n",
    "                for u in k.hypernyms():\n",
    "                    if u.name() in model['entities']:                            \n",
    "                        links[k.name()].add(u.name())\n",
    "                        stack.append(u.name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Function\n",
    "eps = 1e-5\n",
    "class Arcosh(Function):\n",
    "    def __init__(self, eps=eps):\n",
    "        super(Arcosh, self).__init__()\n",
    "        self.eps = eps \n",
    "\n",
    "    def forward(self, x): \n",
    "        self.z = th.sqrt(x * x - 1)\n",
    "        return th.log(x + self.z)\n",
    "\n",
    "    def backward(self, g): \n",
    "        z = th.clamp(self.z, min=eps)\n",
    "        z = g / z \n",
    "        return z\n",
    "\n",
    "class PoincareDistance(Function):\n",
    "    boundary = 1 - eps \n",
    "\n",
    "    def grad(self, x, v, sqnormx, sqnormv, sqdist):\n",
    "        alpha = (1 - sqnormx)\n",
    "        beta = (1 - sqnormv)\n",
    "        z = 1 + 2 * sqdist / (alpha * beta)\n",
    "        a = ((sqnormv - 2 * th.sum(x * v, dim=-1) + 1) / th.pow(alpha, 2)).unsqueeze(-1).expand_as(x)\n",
    "        a = a * x - v / alpha.unsqueeze(-1).expand_as(v)\n",
    "        z = th.sqrt(th.pow(z, 2) - 1)\n",
    "        z = th.clamp(z * beta, min=eps).unsqueeze(-1)\n",
    "        return 4 * a / z.expand_as(x)\n",
    "\n",
    "    def forward(self, u, v): \n",
    "        self.save_for_backward(u, v)\n",
    "        self.squnorm = th.clamp(th.sum(u * u, dim=-1), 0, self.boundary)\n",
    "        self.sqvnorm = th.clamp(th.sum(v * v, dim=-1), 0, self.boundary)\n",
    "        self.sqdist = th.sum(th.pow(u - v, 2), dim=-1)\n",
    "        x = self.sqdist / ((1 - self.squnorm) * (1 - self.sqvnorm)) * 2 + 1 \n",
    "        # arcosh\n",
    "        z = th.sqrt(th.pow(x, 2) - 1)\n",
    "        return th.log(x + z)\n",
    "\n",
    "    def backward(self, g): \n",
    "        u, v = self.saved_tensors\n",
    "        g = g.unsqueeze(-1)\n",
    "        gu = self.grad(u, v, self.squnorm, self.sqvnorm, self.sqdist)\n",
    "        gv = self.grad(v, u, self.sqvnorm, self.squnorm, self.sqdist)\n",
    "        return g.expand_as(gu) * gu, g.expand_as(gv) * gv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import HTML, Image\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import glob\n",
    "import time\n",
    "\n",
    "rc('animation', html='html5')\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_xlim(( -1.1, 1.1))\n",
    "ax.set_ylim((-1.1, 1.1))\n",
    "ax.set_axis_off()\n",
    "\n",
    "bbox_props = dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"black\", lw=1)\n",
    "ld = LorentzDistance()\n",
    "pd = PoincareDistance()\n",
    "s = [1 for n in range(len(model['entities']))]\n",
    "\n",
    "links = {'lynx.n.02': {'feline.n.01'},\n",
    "         'feline.n.01' : {'big_cat.n.01'},\n",
    "         'big_cat.n.01': {'carnivore.n.01'},\n",
    "         'carnivore.n.01': {'mammal.n.01'},\n",
    "         'mammal.n.01': {}}\n",
    "root = model['entities'].index('mammal.n.01')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_items = []\n",
    "circle, = ax.plot([], [], linewidth=1, color='black')\n",
    "fig_items.append(circle)\n",
    "x = []\n",
    "y = []\n",
    "t = np.linspace(0,np.pi*2,1000)\n",
    "x.extend(list(np.cos(t)))\n",
    "y.extend(list(np.sin(t)))\n",
    "\n",
    "scatter = ax.scatter([], [], s=s, color='darkblue')\n",
    "fig_items.append(scatter)\n",
    "\n",
    "label_items = {}\n",
    "link_items = defaultdict(dict)\n",
    "for l in links:\n",
    "    t = ax.text([], [], l, bbox=bbox_props, ha=\"center\", va=\"center\")\n",
    "    fig_items.append(t)\n",
    "    label_items[l] = t\n",
    "    for n in links[l]:\n",
    "        p, = ax.plot([], [], 'k-', lw=0.75)\n",
    "        fig_items.append(p)\n",
    "        link_items[l][n] = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init():\n",
    "    circle.set_data(x, y)\n",
    "    return fig_items,\n",
    "\n",
    "runfiles = list(sorted(glob.glob('./run/*.pth')))[:-1]   \n",
    "\n",
    "def animate(i):\n",
    "    model = th.load(runfiles[i])\n",
    "    lorentz_embeddings = model['model']['embeddings.weight']\n",
    "    dim0 = lorentz_embeddings[:,0].unsqueeze(1)\n",
    "    dimn = lorentz_embeddings[:,1:]\n",
    "\n",
    "    poincare_embeddings = dimn / (dim0 + 1)\n",
    "    #pca = PCA(n_components=2)\n",
    "    #reduced = pca.fit_transform(poincare_embeddings)\n",
    "    reduced = poincare_embeddings\n",
    "    circle.set_data(x, y)\n",
    "    \n",
    "    scatter.set_offsets(reduced)\n",
    "    \n",
    "    for l in links:\n",
    "        start = model['entities'].index(l)\n",
    "        dc = ld(lorentz_embeddings[root].unsqueeze(0), lorentz_embeddings[start].unsqueeze(0)).cpu().data.numpy()\n",
    "        label = \"%s; $d_r$: %.03f\"%(l,dc)\n",
    "        for t in links[l]:\n",
    "            node = model['entities'].index(t)\n",
    "            dp = ld(lorentz_embeddings[node].unsqueeze(0), lorentz_embeddings[start].unsqueeze(0)).cpu().data.numpy()\n",
    "            #ax.text(reduced[node,0], reduced[node,1], lab, bbox=bbox_props, ha=\"center\", va=\"center\")\n",
    "            link_items[l][t].set_data([reduced[start, 0], reduced[node, 0]], [reduced[start, 1], reduced[node, 1]])\n",
    "            label += \";$d_p$: %.03f\"%dp\n",
    "        label_items[l].set_position([reduced[start,0], reduced[start,1]])\n",
    "        label_items[l].set_text(label)\n",
    "\n",
    "    return fig_items,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim = animation.FuncAnimation(fig, animate, init_func=init,\n",
    "                               frames=len(runfiles), interval=100, blit=True)\n",
    "anim.save('./media/visualize.gif', writer='imagemagick', fps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize heatmap of points from the edge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "from sklearn.decomposition import PCA\n",
    "import glob\n",
    "import time\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111)\n",
    "plt.ion()\n",
    "\n",
    "fig.show()\n",
    "fig.canvas.draw()\n",
    "bbox_props = dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"black\", lw=1)\n",
    "ld = LorentzDistance()\n",
    "pd = PoincareDistance()\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "for r in np.linspace(0, 0.99, 100):\n",
    "    t = np.linspace(0,np.pi*2,1000 * r)\n",
    "    x.extend(list(r*np.cos(t)))\n",
    "    y.extend(list(r*np.sin(t)))\n",
    "model = th.load('./mammals.pth')\n",
    "lorentz_embeddings = model['model']['embeddings.weight']\n",
    "dim0 = lorentz_embeddings[:,0].unsqueeze(1)\n",
    "dimn = lorentz_embeddings[:,1:]\n",
    "poincare_embeddings = dimn / (dim0 + 1)\n",
    "p = poincare_embeddings[model['entities'].index('lynx.n.02')]\n",
    "z = [pd(p, th.DoubleTensor([xp,yp]).cuda()).cpu().data.numpy() for xp,yp in zip(x,y)]\n",
    "\n",
    "vmin=min(z)\n",
    "vmax=max(z)\n",
    "\n",
    "x = np.asarray(x)\n",
    "y = np.asarray(y)\n",
    "z = np.asarray(z)\n",
    "x=x.ravel()              #Flat input into 1d vector\n",
    "x=(x[x!=np.isnan])   #eliminate any NaN\n",
    "y=y.ravel()\n",
    "y=(y[y!=np.isnan])\n",
    "z=z.ravel()\n",
    "z=(z[z!=np.isnan])\n",
    "\n",
    "ax.hexbin(x, y, C=z, cmap=plt.cm.jet, bins=None, vmin=0, vmax=vmax)\n",
    "ax.set_axis_off()\n",
    "ax.set_title(\"Heatmap showing the distance from the edge of the space to all other points.\")\n",
    "fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig('distance_from_edge.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize heatmap of points from the center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "from sklearn.decomposition import PCA\n",
    "import glob\n",
    "import time\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111)\n",
    "plt.ion()\n",
    "\n",
    "fig.show()\n",
    "fig.canvas.draw()\n",
    "bbox_props = dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"black\", lw=1)\n",
    "ld = LorentzDistance()\n",
    "pd = PoincareDistance()\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "for r in np.linspace(0, 0.99, 100):\n",
    "    t = np.linspace(0,np.pi*2,1000 * r)\n",
    "    x.extend(list(r*np.cos(t)))\n",
    "    y.extend(list(r*np.sin(t)))\n",
    "model = th.load('./mammals.pth')\n",
    "lorentz_embeddings = model['model']['embeddings.weight']\n",
    "dim0 = lorentz_embeddings[:,0].unsqueeze(1)\n",
    "dimn = lorentz_embeddings[:,1:]\n",
    "poincare_embeddings = dimn / (dim0 + 1)\n",
    "p = th.DoubleTensor([0.0,0.0]).cuda()#poincare_embeddings[model['entities'].index('mammal.n.01')]\n",
    "z = [pd(p, th.DoubleTensor([xp,yp]).cuda()).cpu().data.numpy() for xp,yp in zip(x,y)]\n",
    "\n",
    "x = np.asarray(x)\n",
    "y = np.asarray(y)\n",
    "z = np.asarray(z)\n",
    "x=x.ravel()              #Flat input into 1d vector\n",
    "x=(x[x!=np.isnan])   #eliminate any NaN\n",
    "y=y.ravel()\n",
    "y=(y[y!=np.isnan])\n",
    "z=z.ravel()\n",
    "z=(z[z!=np.isnan])\n",
    "\n",
    "ax.hexbin(x, y, C=z, cmap=plt.cm.jet, bins=None, vmin=0, vmax=vmax)\n",
    "ax.set_title(\"Heatmap showing the distance from the center of the space to all other points.\")\n",
    "ax.set_axis_off()\n",
    "fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig('distance_from_center.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jhubMachineLearning",
   "language": "python",
   "name": "jhubmachinelearning"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
