{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# WordNet dataset generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataForSynset(sname, fname):\n",
    "\n",
    "    closure = defaultdict(set)\n",
    "\n",
    "    def walk(node, ancestors):\n",
    "        node_name = node.name()\n",
    "        closure[node_name].update(ancestors)\n",
    "        for s in node.hyponyms():\n",
    "            walk(s, ancestors + [node_name])\n",
    "\n",
    "    walk(wn.synset(sname), [sname])\n",
    "    #Write out tsv\n",
    "    with open(fname, 'w') as f:\n",
    "        for n in closure:\n",
    "            for a in closure[n]:\n",
    "                f.write(n + '\\t' + a + '\\t' + \"1\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getDataForSynset('entity.n.01', \"data/wordnet_nouns.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getDataForSynset('mammal.n.01', \"data/wordnet_mammals.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import timeit\n",
    "import gc\n",
    "import logging\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.multiprocessing as mp\n",
    "from torch.autograd import Variable\n",
    "from collections import defaultdict as ddict\n",
    "from sklearn.metrics import average_precision_score\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from model import LorentzEmbedding\n",
    "from data import DatasetReader\n",
    "from optimization import RiemannianSGD\n",
    "from optimization import LorentzDistance\n",
    "\n",
    "th.set_default_tensor_type(th.DoubleTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(types, model):\n",
    "    with th.no_grad():\n",
    "        embs = th.from_numpy(model.embedding())\n",
    "        embedding = Variable(embs)\n",
    "        ranks = []\n",
    "        ap_scores = []\n",
    "        for s, tree in types.items():\n",
    "            s_e = Variable(embs[s].unsqueeze(0))\n",
    "            dists_curr = model.dist()(s_e, embedding).data.cpu().numpy().flatten()\n",
    "            dists_curr[s] = 1e14\n",
    "            labels = np.zeros(embedding.size(0))\n",
    "            dists_masked = dists_curr.copy()\n",
    "            ranks_curr = []\n",
    "            \n",
    "            for o in tree:\n",
    "                dists_masked[o] = float('inf')\n",
    "                labels[o] = 1 \n",
    "            ap_scores.append(average_precision_score(labels, -dists_curr))\n",
    "            for o in tree:\n",
    "                d = dists_masked.copy()\n",
    "                d[o] = dists_curr[o]\n",
    "                r = np.argsort(d)\n",
    "                ranks_curr.append(np.where(r == o)[0][0] + 1)\n",
    "            ranks += ranks_curr\n",
    "    return np.mean(ranks), np.mean(ap_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_lr_multiplier = 0.1                     #Burnin multiplier\n",
    "dim = 3                                  #The dimensionality of the embdding\n",
    "dataset = './data/wordnet_mammals.tsv'   #The dataset to learn\n",
    "fout = './mammals.pth'                   #The output model file\n",
    "lr_base = 1.0                            #The base learning rate\n",
    "epochs = 1500                            #Max number of epochs\n",
    "batchsize = 20                           #The batch size\n",
    "negs = 50                                #Number of negative examples\n",
    "eval_each = 10                           #The number of epochs between each evaluation\n",
    "burnin = 20                              #The number of burnin epochs\n",
    "nworkers = 5                             #The number of dataset readers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset setup\n",
    "data = DatasetReader(dataset, negs)\n",
    "idx = data.samples\n",
    "\n",
    "# create adjacency list for evaluation\n",
    "adjacency = ddict(set)\n",
    "for i in range(len(idx)):\n",
    "    s, o, _ = idx[i]\n",
    "    adjacency[s].add(o)\n",
    "adjacency = dict(adjacency)\n",
    "\n",
    "# initialize model and data\n",
    "model = LorentzEmbedding(len(data.entities), dim, LorentzDistance)\n",
    "\n",
    "# initialize optimizer\n",
    "optimizer = RiemannianSGD(\n",
    "    model.parameters(),\n",
    "    lr=lr_base,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dbw003/.conda/envs/jhubMachineLearning/lib/python3.6/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='elementwise_mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/dbw003/.conda/envs/jhubMachineLearning/lib/python3.6/site-packages/ipykernel_launcher.py:32: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9, loss: 1.730, mean rank: 214.94, mAP: 0.0773, best rank: 214.94, best mAP: 0.0773, time: 1.51s\n",
      "epoch: 19, loss: 1.623, mean rank: 202.15, mAP: 0.0829, best rank: 202.15, best mAP: 0.0829, time: 1.35s\n",
      "epoch: 29, loss: 2.975, mean rank: 137.82, mAP: 0.1070, best rank: 137.82, best mAP: 0.1070, time: 2.35s\n",
      "epoch: 39, loss: 2.399, mean rank: 110.76, mAP: 0.1390, best rank: 110.76, best mAP: 0.1390, time: 2.55s\n",
      "epoch: 49, loss: 2.545, mean rank: 91.42, mAP: 0.1550, best rank: 91.42, best mAP: 0.1550, time: 2.25s\n",
      "epoch: 59, loss: 2.715, mean rank: 77.13, mAP: 0.1661, best rank: 77.13, best mAP: 0.1661, time: 2.18s\n",
      "epoch: 69, loss: 1.667, mean rank: 63.91, mAP: 0.1806, best rank: 63.91, best mAP: 0.1806, time: 2.57s\n",
      "epoch: 79, loss: 1.579, mean rank: 54.00, mAP: 0.1939, best rank: 54.00, best mAP: 0.1939, time: 2.51s\n",
      "epoch: 89, loss: 2.509, mean rank: 44.38, mAP: 0.2033, best rank: 44.38, best mAP: 0.2033, time: 2.22s\n",
      "epoch: 99, loss: 0.875, mean rank: 34.73, mAP: 0.2223, best rank: 34.73, best mAP: 0.2223, time: 2.57s\n",
      "epoch: 109, loss: 2.059, mean rank: 25.44, mAP: 0.2486, best rank: 25.44, best mAP: 0.2486, time: 2.30s\n",
      "epoch: 119, loss: 0.674, mean rank: 17.51, mAP: 0.2997, best rank: 17.51, best mAP: 0.2997, time: 2.14s\n",
      "epoch: 129, loss: 1.668, mean rank: 12.53, mAP: 0.3595, best rank: 12.53, best mAP: 0.3595, time: 2.11s\n",
      "epoch: 139, loss: 0.864, mean rank: 9.66, mAP: 0.4191, best rank: 9.66, best mAP: 0.4191, time: 2.44s\n",
      "epoch: 149, loss: 0.728, mean rank: 7.94, mAP: 0.4698, best rank: 7.94, best mAP: 0.4698, time: 2.33s\n",
      "epoch: 159, loss: 0.702, mean rank: 6.90, mAP: 0.5138, best rank: 6.90, best mAP: 0.5138, time: 2.39s\n",
      "epoch: 169, loss: 1.077, mean rank: 6.09, mAP: 0.5494, best rank: 6.09, best mAP: 0.5494, time: 2.18s\n",
      "epoch: 179, loss: 0.454, mean rank: 5.55, mAP: 0.5774, best rank: 5.55, best mAP: 0.5774, time: 2.21s\n",
      "epoch: 189, loss: 0.422, mean rank: 5.15, mAP: 0.6024, best rank: 5.15, best mAP: 0.6024, time: 2.24s\n",
      "epoch: 199, loss: 0.178, mean rank: 4.83, mAP: 0.6247, best rank: 4.83, best mAP: 0.6247, time: 2.58s\n",
      "epoch: 209, loss: 0.514, mean rank: 4.54, mAP: 0.6426, best rank: 4.54, best mAP: 0.6426, time: 2.25s\n",
      "epoch: 219, loss: 0.183, mean rank: 4.36, mAP: 0.6572, best rank: 4.36, best mAP: 0.6572, time: 2.21s\n",
      "epoch: 229, loss: 2.127, mean rank: 4.19, mAP: 0.6724, best rank: 4.19, best mAP: 0.6724, time: 2.29s\n",
      "epoch: 239, loss: 0.662, mean rank: 4.05, mAP: 0.6826, best rank: 4.05, best mAP: 0.6826, time: 2.44s\n",
      "epoch: 249, loss: 2.342, mean rank: 3.93, mAP: 0.6944, best rank: 3.93, best mAP: 0.6944, time: 2.31s\n",
      "epoch: 259, loss: 0.219, mean rank: 3.87, mAP: 0.7029, best rank: 3.87, best mAP: 0.7029, time: 2.24s\n",
      "epoch: 269, loss: 0.421, mean rank: 3.77, mAP: 0.7108, best rank: 3.77, best mAP: 0.7108, time: 2.45s\n",
      "epoch: 279, loss: 0.172, mean rank: 3.67, mAP: 0.7190, best rank: 3.67, best mAP: 0.7190, time: 2.05s\n",
      "epoch: 289, loss: 0.995, mean rank: 3.62, mAP: 0.7242, best rank: 3.62, best mAP: 0.7242, time: 2.51s\n",
      "epoch: 299, loss: 0.103, mean rank: 3.56, mAP: 0.7300, best rank: 3.56, best mAP: 0.7300, time: 2.39s\n",
      "epoch: 309, loss: 2.503, mean rank: 3.51, mAP: 0.7328, best rank: 3.51, best mAP: 0.7328, time: 2.55s\n",
      "epoch: 319, loss: 0.087, mean rank: 3.46, mAP: 0.7393, best rank: 3.46, best mAP: 0.7393, time: 2.37s\n",
      "epoch: 329, loss: 0.161, mean rank: 3.43, mAP: 0.7446, best rank: 3.43, best mAP: 0.7446, time: 2.33s\n",
      "epoch: 339, loss: 0.453, mean rank: 3.42, mAP: 0.7476, best rank: 3.42, best mAP: 0.7476, time: 2.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-1717:\n",
      "Process Process-1720:\n",
      "Process Process-1719:\n",
      "Process Process-1718:\n",
      "Process Process-1716:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dbw003/.conda/envs/jhubMachineLearning/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/dbw003/.conda/envs/jhubMachineLearning/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/dbw003/.conda/envs/jhubMachineLearning/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dbw003/.conda/envs/jhubMachineLearning/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/dbw003/.conda/envs/jhubMachineLearning/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/dbw003/.conda/envs/jhubMachineLearning/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dbw003/.conda/envs/jhubMachineLearning/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/dbw003/.conda/envs/jhubMachineLearning/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 110, in _worker_loop\n",
      "    data_queue.put((idx, samples))\n",
      "  File \"/home/dbw003/.conda/envs/jhubMachineLearning/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 110, in _worker_loop\n",
      "    data_queue.put((idx, samples))\n",
      "  File \"/home/dbw003/.conda/envs/jhubMachineLearning/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/dbw003/.conda/envs/jhubMachineLearning/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/dbw003/.conda/envs/jhubMachineLearning/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/dbw003/.conda/envs/jhubMachineLearning/lib/python3.6/multiprocessing/queues.py\", line 346, in put\n",
      "    with self._wlock:\n",
      "  File \"/home/dbw003/.conda/envs/jhubMachineLearning/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/dbw003/.conda/envs/jhubMachineLearning/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 110, in _worker_loop\n",
      "    data_queue.put((idx, samples))\n",
      "  File \"/home/dbw003/.conda/envs/jhubMachineLearning/lib/python3.6/multiprocessing/queues.py\", line 347, in put\n",
      "    self._writer.send_bytes(obj)\n",
      "  File \"/home/dbw003/.conda/envs/jhubMachineLearning/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/dbw003/.conda/envs/jhubMachineLearning/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/dbw003/jupyterhub/lorentz_embeddings_recreation/data.py\", line 56, in __getitem__\n",
      "    if self.weights[left][n] < weight:\n",
      "  File \"/home/dbw003/.conda/envs/jhubMachineLearning/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 110, in _worker_loop\n",
      "    data_queue.put((idx, samples))\n",
      "  File \"/home/dbw003/jupyterhub/lorentz_embeddings_recreation/data.py\", line 16, in <lambda>\n",
      "    self.weights = defaultdict(lambda: defaultdict(lambda: 0))\n",
      "  File \"/home/dbw003/.conda/envs/jhubMachineLearning/lib/python3.6/multiprocessing/queues.py\", line 346, in put\n",
      "    with self._wlock:\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/dbw003/.conda/envs/jhubMachineLearning/lib/python3.6/multiprocessing/queues.py\", line 346, in put\n",
      "    with self._wlock:\n",
      "  File \"/home/dbw003/.conda/envs/jhubMachineLearning/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/dbw003/.conda/envs/jhubMachineLearning/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/dbw003/.conda/envs/jhubMachineLearning/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/dbw003/.conda/envs/jhubMachineLearning/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/dbw003/.conda/envs/jhubMachineLearning/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "KeyboardInterrupt\n",
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f1f5882fe48>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dbw003/.conda/envs/jhubMachineLearning/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 399, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/dbw003/.conda/envs/jhubMachineLearning/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 378, in _shutdown_workers\n",
      "    self.worker_result_queue.get()\n",
      "  File \"/home/dbw003/.conda/envs/jhubMachineLearning/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/dbw003/.conda/envs/jhubMachineLearning/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/dbw003/.conda/envs/jhubMachineLearning/lib/python3.6/multiprocessing/connection.py\", line 411, in _recv_bytes\n",
      "    return self._recv(size)\n",
      "  File \"/home/dbw003/.conda/envs/jhubMachineLearning/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/dbw003/.conda/envs/jhubMachineLearning/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 227, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 7849) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-efae76d51880>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m#Training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jhubMachineLearning/lib/python3.6/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \"\"\"\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "min_rank = (float('inf'), -1)\n",
    "max_map = (0, -1)\n",
    "loader = DataLoader(\n",
    "        data,\n",
    "        batch_size=batchsize,\n",
    "        shuffle=True,\n",
    "        num_workers=nworkers,\n",
    "        collate_fn=data.collate\n",
    "    )\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = []\n",
    "    loss = None\n",
    "    data.burnin = False\n",
    "    lr = lr_base\n",
    "    t_start = timeit.default_timer()\n",
    "    if epoch < burnin:\n",
    "        data.burnin = True\n",
    "        lr = lr_base * _lr_multiplier\n",
    "        \n",
    "    #Training loop\n",
    "    for inputs, targets in loader:\n",
    "        inputs = Variable(th.from_numpy(np.vstack(inputs)))\n",
    "        targets = Variable(th.from_numpy(np.vstack(targets))).squeeze()\n",
    "\n",
    "        elapsed = timeit.default_timer() - t_start\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(inputs)\n",
    "        loss = model.loss(preds, targets, size_average=True)\n",
    "        loss.backward()\n",
    "        optimizer.step(lr=lr)\n",
    "        epoch_loss.append(loss.data[0])\n",
    "        th.save({\n",
    "           'model': model.state_dict(),\n",
    "           'epoch': epoch,\n",
    "           'entities': data.entities\n",
    "        }, 'run/%05d.pth'%epoch)\n",
    "        \n",
    "    #Evaluation\n",
    "    if epoch == (epochs - 1) or epoch % eval_each == (eval_each - 1):\n",
    "        th.save({\n",
    "                'model': model.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'entities': data.entities\n",
    "            }, fout)\n",
    "        mrank, mAP = evaluate(adjacency, model)\n",
    "        if mrank < min_rank[0]:\n",
    "            min_rank = (mrank, epoch)\n",
    "        if mAP > max_map[0]:\n",
    "            max_map = (mAP, epoch)\n",
    "        print(\n",
    "            ('epoch: %d, '\n",
    "             'loss: %.3f, '\n",
    "             'mean rank: %.2f, '\n",
    "             'mAP: %.4f, '\n",
    "             'best rank: %.2f, '\n",
    "             'best mAP: %.4f, '\n",
    "             'time: %.2fs') % (\n",
    "                 epoch, loss, mrank, mAP, min_rank[0], max_map[0], elapsed)\n",
    "        )\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = th.load('./mammals.pth')\n",
    "lorentz_embeddings = model['model']['embeddings.weight']\n",
    "dim0 = lorentz_embeddings[:,0].unsqueeze(1)\n",
    "dimn = lorentz_embeddings[:,1:]\n",
    "\n",
    "poincare_embeddings = dimn / (dim0 + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = defaultdict(set)\n",
    "family_start = ['lynx.n.02']\n",
    "for f in family_start:\n",
    "    stack = [f]\n",
    "    first = True\n",
    "    while len(stack) > 0:\n",
    "        v = stack.pop()\n",
    "        for k in wn.synsets(v.split('.')[0]):\n",
    "            if k.name() in model['entities']:\n",
    "                for u in k.hypernyms():\n",
    "                    if u.name() in model['entities']:                            \n",
    "                        links[k.name()].add(u.name())\n",
    "                        stack.append(u.name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Function\n",
    "eps = 1e-5\n",
    "class Arcosh(Function):\n",
    "    def __init__(self, eps=eps):\n",
    "        super(Arcosh, self).__init__()\n",
    "        self.eps = eps \n",
    "\n",
    "    def forward(self, x): \n",
    "        self.z = th.sqrt(x * x - 1)\n",
    "        return th.log(x + self.z)\n",
    "\n",
    "    def backward(self, g): \n",
    "        z = th.clamp(self.z, min=eps)\n",
    "        z = g / z \n",
    "        return z\n",
    "\n",
    "class PoincareDistance(Function):\n",
    "    boundary = 1 - eps \n",
    "\n",
    "    def grad(self, x, v, sqnormx, sqnormv, sqdist):\n",
    "        alpha = (1 - sqnormx)\n",
    "        beta = (1 - sqnormv)\n",
    "        z = 1 + 2 * sqdist / (alpha * beta)\n",
    "        a = ((sqnormv - 2 * th.sum(x * v, dim=-1) + 1) / th.pow(alpha, 2)).unsqueeze(-1).expand_as(x)\n",
    "        a = a * x - v / alpha.unsqueeze(-1).expand_as(v)\n",
    "        z = th.sqrt(th.pow(z, 2) - 1)\n",
    "        z = th.clamp(z * beta, min=eps).unsqueeze(-1)\n",
    "        return 4 * a / z.expand_as(x)\n",
    "\n",
    "    def forward(self, u, v): \n",
    "        self.save_for_backward(u, v)\n",
    "        self.squnorm = th.clamp(th.sum(u * u, dim=-1), 0, self.boundary)\n",
    "        self.sqvnorm = th.clamp(th.sum(v * v, dim=-1), 0, self.boundary)\n",
    "        self.sqdist = th.sum(th.pow(u - v, 2), dim=-1)\n",
    "        x = self.sqdist / ((1 - self.squnorm) * (1 - self.sqvnorm)) * 2 + 1 \n",
    "        # arcosh\n",
    "        z = th.sqrt(th.pow(x, 2) - 1)\n",
    "        return th.log(x + z)\n",
    "\n",
    "    def backward(self, g): \n",
    "        u, v = self.saved_tensors\n",
    "        g = g.unsqueeze(-1)\n",
    "        gu = self.grad(u, v, self.squnorm, self.sqvnorm, self.sqdist)\n",
    "        gv = self.grad(v, u, self.sqvnorm, self.squnorm, self.sqdist)\n",
    "        return g.expand_as(gu) * gu, g.expand_as(gv) * gv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import HTML, Image\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import glob\n",
    "import time\n",
    "\n",
    "rc('animation', html='html5')\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_xlim(( -1.1, 1.1))\n",
    "ax.set_ylim((-1.1, 1.1))\n",
    "ax.set_axis_off()\n",
    "\n",
    "bbox_props = dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"black\", lw=1)\n",
    "ld = LorentzDistance()\n",
    "pd = PoincareDistance()\n",
    "s = [1 for n in range(len(model['entities']))]\n",
    "\n",
    "links = {'lynx.n.02': {'feline.n.01'},\n",
    "         'feline.n.01' : {'big_cat.n.01'},\n",
    "         'big_cat.n.01': {'carnivore.n.01'},\n",
    "         'carnivore.n.01': {'mammal.n.01'},\n",
    "         'mammal.n.01': {}}\n",
    "root = model['entities'].index('mammal.n.01')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_items = []\n",
    "circle, = ax.plot([], [], linewidth=1, color='black')\n",
    "fig_items.append(circle)\n",
    "x = []\n",
    "y = []\n",
    "t = np.linspace(0,np.pi*2,1000)\n",
    "x.extend(list(np.cos(t)))\n",
    "y.extend(list(np.sin(t)))\n",
    "\n",
    "scatter = ax.scatter([], [], s=s, color='darkblue')\n",
    "fig_items.append(scatter)\n",
    "\n",
    "label_items = {}\n",
    "link_items = defaultdict(dict)\n",
    "for l in links:\n",
    "    t = ax.text([], [], l, bbox=bbox_props, ha=\"center\", va=\"center\")\n",
    "    fig_items.append(t)\n",
    "    label_items[l] = t\n",
    "    for n in links[l]:\n",
    "        p, = ax.plot([], [], 'k-', lw=0.75)\n",
    "        fig_items.append(p)\n",
    "        link_items[l][n] = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "from sklearn.decomposition import PCA\n",
    "import glob\n",
    "import time\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111)\n",
    "plt.ion()\n",
    "\n",
    "fig.show()\n",
    "fig.canvas.draw()\n",
    "bbox_props = dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"black\", lw=1)\n",
    "ld = LorentzDistance()\n",
    "pd = PoincareDistance()\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "for r in np.linspace(0, 0.99, 100):\n",
    "    t = np.linspace(0,np.pi*2,1000 * r)\n",
    "    x.extend(list(r*np.cos(t)))\n",
    "    y.extend(list(r*np.sin(t)))\n",
    "model = th.load('./mammals.pth')\n",
    "lorentz_embeddings = model['model']['embeddings.weight']\n",
    "dim0 = lorentz_embeddings[:,0].unsqueeze(1)\n",
    "dimn = lorentz_embeddings[:,1:]\n",
    "poincare_embeddings = dimn / (dim0 + 1)\n",
    "p = poincare_embeddings[model['entities'].index('lynx.n.02')]\n",
    "z = [pd(p, th.DoubleTensor([xp,yp]).cuda()).cpu().data.numpy() for xp,yp in zip(x,y)]\n",
    "\n",
    "vmin=min(z)\n",
    "vmax=max(z)\n",
    "\n",
    "x = np.asarray(x)\n",
    "y = np.asarray(y)\n",
    "z = np.asarray(z)\n",
    "x=x.ravel()              #Flat input into 1d vector\n",
    "x=(x[x!=np.isnan])   #eliminate any NaN\n",
    "y=y.ravel()\n",
    "y=(y[y!=np.isnan])\n",
    "z=z.ravel()\n",
    "z=(z[z!=np.isnan])\n",
    "\n",
    "ax.hexbin(x, y, C=z, cmap=plt.cm.jet, bins=None, vmin=0, vmax=vmax)\n",
    "ax.set_axis_off()\n",
    "ax.set_title(\"Heatmap showing the distance from the edge of the space to all other points.\")\n",
    "fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig('distance_from_edge.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "from sklearn.decomposition import PCA\n",
    "import glob\n",
    "import time\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111)\n",
    "plt.ion()\n",
    "\n",
    "fig.show()\n",
    "fig.canvas.draw()\n",
    "bbox_props = dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"black\", lw=1)\n",
    "ld = LorentzDistance()\n",
    "pd = PoincareDistance()\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "for r in np.linspace(0, 0.99, 100):\n",
    "    t = np.linspace(0,np.pi*2,1000 * r)\n",
    "    x.extend(list(r*np.cos(t)))\n",
    "    y.extend(list(r*np.sin(t)))\n",
    "model = th.load('./mammals.pth')\n",
    "lorentz_embeddings = model['model']['embeddings.weight']\n",
    "dim0 = lorentz_embeddings[:,0].unsqueeze(1)\n",
    "dimn = lorentz_embeddings[:,1:]\n",
    "poincare_embeddings = dimn / (dim0 + 1)\n",
    "p = th.DoubleTensor([0.0,0.0]).cuda()#poincare_embeddings[model['entities'].index('mammal.n.01')]\n",
    "z = [pd(p, th.DoubleTensor([xp,yp]).cuda()).cpu().data.numpy() for xp,yp in zip(x,y)]\n",
    "\n",
    "x = np.asarray(x)\n",
    "y = np.asarray(y)\n",
    "z = np.asarray(z)\n",
    "x=x.ravel()              #Flat input into 1d vector\n",
    "x=(x[x!=np.isnan])   #eliminate any NaN\n",
    "y=y.ravel()\n",
    "y=(y[y!=np.isnan])\n",
    "z=z.ravel()\n",
    "z=(z[z!=np.isnan])\n",
    "\n",
    "ax.hexbin(x, y, C=z, cmap=plt.cm.jet, bins=None, vmin=0, vmax=vmax)\n",
    "ax.set_title(\"Heatmap showing the distance from the center of the space to all other points.\")\n",
    "ax.set_axis_off()\n",
    "fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig('distance_from_center.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jhubMachineLearning",
   "language": "python",
   "name": "jhubmachinelearning"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
